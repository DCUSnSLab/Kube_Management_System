from pathlib import Path
import re
from typing import Dict, List, Tuple
import pandas as pd
from pathlib import PurePosixPath

from tool.data_graph import main

FILENAME_REGEX = re.compile(r"^process_metrics_experiment(\d+)\.csv$", re.IGNORECASE)

def find_experiment_files(dir_path: str | Path) -> List[Path]:
    """
    ì£¼ì–´ì§„ ë””ë ‰í„°ë¦¬ì—ì„œ 'process_metrics_experiment*.csv' íŒŒì¼ì„ ëª¨ë‘ ì°¾ê³ ,
    íŒŒì¼ëª… ë’¤ì˜ ì‹¤í—˜ ë²ˆí˜¸(ì •ìˆ˜) ê¸°ì¤€ìœ¼ë¡œ ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬í•˜ì—¬ Path ë¦¬ìŠ¤íŠ¸ ë°˜í™˜.
    """
    base = Path(dir_path)
    if not base.is_dir():
        raise NotADirectoryError(f"Not a directory: {base}")

    candidates = list(base.glob("process_metrics_experiment*.csv"))

    # (exp_no, Path) íŠœí”Œë¡œ ì •ë¦¬ í›„ ì •ë ¬
    parsed: List[Tuple[int, Path]] = []
    for p in candidates:
        m = FILENAME_REGEX.match(p.name)
        if m:
            exp_no = int(m.group(1))
            parsed.append((exp_no, p))
        else:
            # íŒ¨í„´ì— ì•ˆ ë§ëŠ” ê²½ìš°ëŠ” ìŠ¤í‚µ (ì›í•˜ë©´ ë¡œê·¸ ì¶œë ¥ ê°€ëŠ¥)
            pass

    # ì‹¤í—˜ ë²ˆí˜¸ ê¸°ì¤€ ì •ë ¬
    parsed.sort(key=lambda x: x[0])

    # Pathë§Œ ì¶”ì¶œ
    return [p for _, p in parsed]

def load_experiment_csvs(dir_path: str | Path, **read_csv_kwargs) -> Dict[int, pd.DataFrame]:
    """
    í•´ë‹¹ ë””ë ‰í„°ë¦¬ì˜ ì‹¤í—˜ CSVë“¤ì„ ëª¨ë‘ ì½ì–´ì„œ {ì‹¤í—˜ë²ˆí˜¸: DataFrame} í˜•íƒœë¡œ ë°˜í™˜.
    read_csv_kwargsë¡œ encoding='utf-8', dtype=..., usecols=... ê°™ì€ ì˜µì…˜ì„ ì „ë‹¬ ê°€ëŠ¥.
    """
    files = find_experiment_files(dir_path)
    result: Dict[int, pd.DataFrame] = {}

    for p in files:
        m = FILENAME_REGEX.match(p.name)
        exp_no = int(m.group(1))  # ì •ê·œì‹ ë§¤ì¹˜ê°€ ë³´ì¥ë¨
        try:
            df = pd.read_csv(p, **read_csv_kwargs)
            result[exp_no] = df
        except Exception as e:
            # íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ ì‹œ: ìƒí™©ì— ë”°ë¼ raise í•˜ê±°ë‚˜ ìŠ¤í‚µí•˜ë„ë¡ ì„ íƒ
            raise RuntimeError(f"Failed to read {p}: {e}") from e

    return result

def clean_metricData(datasets: Dict[int, pd.DataFrame]) -> Dict[int, pd.DataFrame]:
    """
    datasets ë‚´ ëª¨ë“  DataFrameì—ì„œ pid == 1 ì´ê³ 
    comm == "/bin/bash /entrypoint.sh" ì¸ rowë¥¼ ì‚­ì œí•˜ì—¬ ë°˜í™˜.
    ì›ë³¸ datasetsëŠ” ë³€ê²½í•˜ì§€ ì•Šê³ , ìƒˆë¡œìš´ Dict ë°˜í™˜.
    """
    cleaned: Dict[int, pd.DataFrame] = {}

    for exp_no, df in datasets.items():
        before_count = len(df)
        df_cleaned = df.drop(
            df[(df["pid"] == 1) & (df["comm"] == "/bin/bash /entrypoint.sh")].index
        )
        after_count = len(df_cleaned)

        print(f"Experiment {exp_no}: {before_count} â†’ {after_count} rows (ì‚­ì œ {before_count - after_count})")
        cleaned[exp_no] = df_cleaned.reset_index(drop=True)

    return cleaned

def showDF(df: pd.DataFrame) -> None:
    """
    ì£¼ì–´ì§„ DataFrameì—ì„œ pod_name, timestamp, pid, comm ì»¬ëŸ¼ë§Œ ì¶œë ¥
    """
    required_cols = ["pod_name", "timestamp", "pid", "comm"]

    # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ (í˜¹ì‹œ ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ë¬´ì‹œ)
    available_cols = [c for c in required_cols if c in df.columns]
    if not available_cols:
        print("âš ï¸ ì¶œë ¥í•  ìˆ˜ ìˆëŠ” ì§€ì • ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ì•ë¶€ë¶„ 5ê°œë§Œ ì¶œë ¥ (ì „ì²´ ì¶œë ¥ì€ df[available_cols].to_string(index=False) ì‚¬ìš© ê°€ëŠ¥)
    print("\n=== ì„ íƒëœ ì»¬ëŸ¼ ì¶œë ¥ ===")
    print(df[available_cols].head(10).to_string(index=False))


def show_active0(df: pd.DataFrame) -> None:
    """
    DataFrameì—ì„œ pod_name == 'active-0' ì¸ rowë§Œ ì¶œë ¥
    """
    if "pod_name" not in df.columns:
        print("âš ï¸ DataFrameì— 'pod_name' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    filtered = df[df["pod_name"] == "active-0"]

    if filtered.empty:
        print("âš ï¸ pod_name == 'active-0' ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        print(f"\n=== pod_name == 'active-0' ë°ì´í„° ({len(filtered)} rows) ===")
        print(filtered.to_string(index=False))  # ì•ë¶€ë¶„ 10ì¤„ë§Œ ì¶œë ¥

def showAll(df: pd.DataFrame) -> None:
    """
    DataFrameì˜ ì „ì²´ ë°ì´í„°ë¥¼ ëª¨ë‘ ì¶œë ¥í•˜ê³ ,
    row ê°œìˆ˜ë¥¼ í•¨ê»˜ ì¶œë ¥í•œë‹¤.
    """
    if df is None or df.empty:
        print("âš ï¸ DataFrameì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        return

    print(f"\n=== DataFrame ì „ì²´ ì¶œë ¥ (ì´ {len(df)} rows) ===")
    print(df.to_string(index=False))  # ì „ì²´ row ì¶œë ¥

def save_to_excel(df: pd.DataFrame, file_path: str, sheet_name: str = "Sheet1") -> None:
    """
    DataFrameì„ Excel íŒŒì¼(.xlsx)ë¡œ ì €ì¥í•˜ëŠ” í•¨ìˆ˜
    """
    if df is None or df.empty:
        print("âš ï¸ ì €ì¥í•  DataFrameì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        return

    try:
        df.to_excel(file_path, sheet_name=sheet_name, index=False, engine="openpyxl")
        print(f"âœ… DataFrameì´ Excel íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {file_path}")
    except Exception as e:
        print(f"âŒ Excel ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

import re
import pandas as pd
from pathlib import PurePosixPath

def _split_pod_base_and_ordinal(name: str):
    """
    'active-0' -> ('active', 0)
    'worker-12' -> ('worker', 12)
    'singleton' -> ('singleton', None)
    """
    if not isinstance(name, str):
        return name, None
    m = re.match(r"^(.*?)-(\d+)$", name.strip())
    if m:
        base, idx = m.group(1), int(m.group(2))
        return base, idx
    return name.strip(), None

def _extract_comm_tail(comm: str) -> str:
    """
    commì˜ ë§ˆì§€ë§‰ í† í°ì—ì„œ íŒŒì¼ëª… stem ì¶”ì¶œ
    ì˜ˆ) 'python ./programs/active/active_multithreaded.py' -> 'active_multithreaded'
    """
    if not isinstance(comm, str) or not comm.strip():
        return comm
    tokens = comm.strip().split()
    chosen = None
    for tok in reversed(tokens):
        if "/" in tok or "." in tok:
            chosen = tok
            break
    if chosen is None:
        chosen = tokens[-1]
    chosen = chosen.strip('\'"')
    stem = PurePosixPath(chosen).name
    if "." in stem:
        stem = stem.split(".")[0]
    return stem

def build_normalized_usage_table(
    df: pd.DataFrame,
    ticks_per_sec: int = 100,
    page_size: int = 4096,
    *,
    cycle_size: int = 100,  # CSV/ì…ë ¥ ìˆœì„œëŒ€ë¡œ cycle_sizeê°œì”© í•œ ì‚¬ì´í´
) -> pd.DataFrame:
    """
    - pod_name: ëì˜ -ìˆ«ì ì œê±°í•˜ì—¬ ê¸°ë³¸ëª…ìœ¼ë¡œ ì¬ì •ì˜, ìˆ«ìëŠ” pod_ordinalë¡œ ë¶„ë¦¬
    - comm: ë§ˆì§€ë§‰ íŒŒì¼ëª…ì˜ stemìœ¼ë¡œ ë¶„ë¥˜
    - ì €ì¥ ì»¬ëŸ¼:
      pod_name, pod_ordinal, timestamp, comm, state,
      utime, stime, cutime, num_threads,
      vsize, rss, rsslim, vm_rss_status,
      voluntary_ctxt_switches, nonvoluntary_ctxt_switches,
      read_bytes, write_bytes
    - ëˆ„ì ê°’ì— ëŒ€í•´ *_delta ìƒì„±:
      utime, stime, cutime,
      voluntary_ctxt_switches, nonvoluntary_ctxt_switches,
      read_bytes, write_bytes,
      vsize, rss, rsslim
    - ë¸íƒ€ ê³„ì‚°ì€ (pod_name_base, pod_ordinal, pid) ë‹¨ìœ„ë¡œ ì •ë ¬Â·ì°¨ë¶„
    - CSV ì…ë ¥ ìˆœì„œëŒ€ë¡œ cycle_id ë¶€ì—¬(0ë¶€í„°), í•œ ì‚¬ì´í´ì€ cycle_sizeê°œ ë ˆì½”ë“œ
    """
    required = [
        "pod_name", "timestamp", "comm", "state", "pid",
        "utime", "stime", "cutime", "minflt", "majflt",
        "num_threads",
        "vsize", "rss", "rsslim", "vm_rss_status",
        "voluntary_ctxt_switches", "nonvoluntary_ctxt_switches",
        "read_bytes", "write_bytes"
    ]
    missing = [c for c in required if c not in df.columns]
    if missing:
        raise KeyError(f"í•„ìˆ˜ ì»¬ëŸ¼ ëˆ„ë½: {missing}")
    if cycle_size is None or cycle_size <= 0:
        raise ValueError("cycle_sizeëŠ” 1 ì´ìƒì˜ ì •ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤.")

    # --- ì›ë³¸ ìˆœì„œ ë³´ì¡´ìš© ìœ„ì¹˜ ---
    out = df.copy()
    out["_row_pos"] = range(len(out))  # CSV/ì…ë ¥ ìˆœì„œ ì¸ë±ìŠ¤

    # 1) pod_name ë¶„í•´ â†’ base, ordinal
    base_and_idx = out["pod_name"].astype(str).apply(_split_pod_base_and_ordinal)
    out["pod_name"] = base_and_idx.apply(lambda x: x[0])
    out["pod_ordinal"] = base_and_idx.apply(lambda x: x[1])

    # pod_ordinalì´ Noneì´ë©´ ê·¸ë£¹ì—ì„œ ëˆ„ë½ë˜ì§€ ì•Šë„ë¡ -1ë¡œ ì¹˜í™˜
    out["pod_ordinal"] = out["pod_ordinal"].fillna(-1).astype("int64")

    # 2) comm ì •ê·œí™”
    out["comm"] = out["comm"].astype(str).map(_extract_comm_tail)

    # 3) timestamp ì •ê·œí™”
    out["timestamp"] = pd.to_datetime(out["timestamp"], errors="coerce")
    out = out.dropna(subset=["timestamp", "pid"]).copy()

    # ëˆ„ì ê°’ ì»¬ëŸ¼ ì •ì˜
    cumulative_cols = [
        "utime", "stime", "cutime", "minflt", "majflt",
        "voluntary_ctxt_switches", "nonvoluntary_ctxt_switches",
        "read_bytes", "write_bytes",
        "vsize", "rss", "rsslim"
    ]
    for col in cumulative_cols:
        out[col] = pd.to_numeric(out[col], errors="coerce")

    # ì •ë ¬ëœ ë³µì‚¬ë³¸ìœ¼ë¡œ ë¸íƒ€ ê³„ì‚°
    sort_keys = ["pod_name", "pod_ordinal", "pid", "timestamp", "_row_pos"]
    tmp = out.sort_values(sort_keys).copy()
    gkeys = ["pod_name", "pod_ordinal", "pid"]

    for col in cumulative_cols:
        dcol = f"{col}_delta"
        tmp[dcol] = (
            tmp.groupby(gkeys, sort=False)[col]
            .diff()
            .clip(lower=0)
            .fillna(0.0)
            .astype("float64")
        )

    # --- cpu_time(utime+stime) ê³„ì‚° ---
    # ğŸ”¹ utime+stime = cpu_time ê³„ì‚°
    tmp["cpu_time"] = tmp["utime"] + tmp["stime"]

    # ğŸ”¹ cpu_time_delta ê³„ì‚°
    tmp["cpu_time_delta"] = (
        tmp.groupby(gkeys, sort=False)["cpu_time"]
        .diff()
        .clip(lower=0)
        .fillna(0.0)
        .astype("float64")
    )

    # --- cpu_rate ê³„ì‚° ---
    # curr_time = utime_delta + stime_delta
    tmp["cpu_time_sum"] = tmp["utime_delta"] + tmp["stime_delta"]

    # prev_time = ì´ì „ í–‰ì˜ cpu_time_sum
    tmp["prev_cpu_time_sum"] = tmp.groupby(gkeys, sort=False)["cpu_time_sum"].shift(1)

    # (curr - prev) / prev * 100
    tmp["cpu_rate"] = (
                              (tmp["cpu_time_sum"] - tmp["prev_cpu_time_sum"]) / tmp["prev_cpu_time_sum"]
                      ) * 100

    # ì²« í–‰ì€ NaN -> 0ìœ¼ë¡œ ì±„ì›€
    tmp["cpu_rate"] = tmp["cpu_rate"].fillna(0.0)

    # ë¸íƒ€ ë° cpu_rate, cpu_time, cpu_time_delta ì›ë˜ ìˆœì„œë¡œ ë¶™ì´ê¸°
    extra_cols = [f"{c}_delta" for c in cumulative_cols] + [
        "cpu_rate", "cpu_time", "cpu_time_delta"
    ]
    out = out.merge(tmp[["_row_pos"] + extra_cols], on="_row_pos", how="left")

    # cycle_id ë¶€ì—¬
    out["cycle_id"] = (out["_row_pos"] // int(cycle_size)).astype("int64")

    # ìµœì¢… ì»¬ëŸ¼ ì •ë¦¬
    keep_cols = [
        "cycle_id",
        "pod_name", "pod_ordinal", "timestamp", "comm", "state",
        "utime", "utime_delta",
        "stime", "stime_delta",
        "cutime", "cutime_delta",
        "cpu_time", "cpu_time_delta",
        "minflt", "minflt_delta", "majflt", "majflt_delta",
        "num_threads",
        "vsize", "vsize_delta",
        "rss", "rss_delta",
        "rsslim", "rsslim_delta",
        "vm_rss_status",
        "voluntary_ctxt_switches", "voluntary_ctxt_switches_delta",
        "nonvoluntary_ctxt_switches", "nonvoluntary_ctxt_switches_delta",
        "read_bytes", "read_bytes_delta",
        "write_bytes", "write_bytes_delta",
        "cpu_rate",  # ğŸ‘ˆ ì¶”ê°€ë¨
        "pid",
    ]
    out = out[keep_cols + (["_row_pos"] if "_row_pos" in out.columns else [])] \
        .sort_values(["cycle_id", "_row_pos"]) \
        .reset_index(drop=True)

    return out


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ì˜ˆ: í˜„ì¬ í´ë” ê¸°ì¤€
    dir_path = "experiment_data/"  # ì‘ì—… ë””ë ‰í„°ë¦¬ ê²½ë¡œë¡œ ë°”ê¿”ì£¼ì„¸ìš”
    files = find_experiment_files(dir_path)
    print("ë°œê²¬ëœ íŒŒì¼:")
    for f in files:
        print(" -", f.name)

    # CSV ë¡œë“œ (í•„ìš”í•˜ë©´ encoding, dtype ë“±ì„ ì§€ì •í•˜ì„¸ìš”)
    datasets = load_experiment_csvs(dir_path, encoding="utf-8")
    print(f"\nì´ {len(datasets)}ê°œ ì‹¤í—˜ ë°ì´í„° ë¡œë“œ ì™„ë£Œ.")
    # ì˜ˆ: 1ë²ˆ ì‹¤í—˜ ë°ì´í„° ì •ë³´ ì¶œë ¥
    if 1 in datasets:
        print("exp 1 shape:", datasets[1].shape)
        df = datasets[1]
        for col in df.columns:
            print(col, end=', ')

    datasets = clean_metricData(datasets)
    #showDF(datasets[1])
    #show_active0(datasets[1])

    normal_datas = dict()
    for i, dataset in enumerate(datasets):
        normal_datas['experiment_'+str(i)] = (build_normalized_usage_table(datasets[i+1]))
    #showAll(df_usage)
    save_to_excel(normal_datas['experiment_'+str(0)], "usage1.xlsx")
    print(normal_datas.keys())
    main(normal_datas)